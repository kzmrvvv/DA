# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #5 выполнил:
- Казимиров Кирилл Евгеньевич
- НМТ-211506
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 80 |
| Задание 2 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

## Цель работы
Интегрировать экономическую систему в проект Unity и обучить ML-Agent.

## Задание 1
### Изменить параметры файла .yaml-агента и определить какие параметры и как влияют на обучение модели.

Прежде чем менять параметры конфигурации нейронной сети, было бы неплохо обучить её на предоставленных данных, чтобы в будущем сравнить с изменёнными.

После загрузки проекта и установки mlagents packages я начал обучение нейронки.

![1](https://user-images.githubusercontent.com/114439735/207908923-066324ab-55ba-4f9b-a89d-a9487cc471c3.jpg)

Очевидно, что процесс обучения пойдёт быстрее, если добавить больше префабов, на которых сетка учится.

![2](https://user-images.githubusercontent.com/114439735/207908939-b3f99087-04fc-42e6-b7be-0616c6a33fed.jpg)

Спустя недолгое время обучение было завершено (а точнее прервано мной),

![3](https://user-images.githubusercontent.com/114439735/207908955-7c128ce7-8811-4086-9054-cec16de715c3.jpg)

и на основании полученных данных TensorBoard любезно построил следующие графики:

![4](https://user-images.githubusercontent.com/114439735/207908980-181542f5-7b8d-4323-a2a2-6ade1f4df68c.jpg)

![5](https://user-images.githubusercontent.com/114439735/207909020-008ed4dd-427f-4ee6-ac6f-adb9b36e931c.jpg)

Нейросеть обучалась, следуя этой конфигурации:

![6](https://user-images.githubusercontent.com/114439735/207909079-825f948f-4479-418b-9ad2-965371ae0563.jpg)

И вот настал момент, когда можно залезть в .yaml файл и намешать там грязи! Однако, не желая всё сломать, а потом чинить несколько часов (хотя что тут вообще можно сломать), я добавил лишь небольшие не особо осознанные корректировки:

#### buffer_size: 10_240 => 15_000
#### beta: 1.0e-2 => 5.0e-1
#### checkpoint_interval: 500_000 => 50_000
#### max_steps: 750_000 => 100_000
#### time_horizon: 64 => 128
#### summary_freq: 5000 => 0

За что отвечает каждый из этих параметров? Ответ в задании №2.

![7](https://user-images.githubusercontent.com/114439735/207909126-d93a475f-2d45-4356-af18-fcd618abd0a4.jpg)

После внесённых изменений я начал обучение вновь, но, как оказалось, всё-таки что-то сломал:

![8](https://user-images.githubusercontent.com/114439735/207909183-b320b9bd-acfe-40e0-be17-f3fb4d25969b.jpg)

Изменение параметра summary_freq с 5000 до 0 не понравилось mlagent, в какой-то момент во время обучения он делил на этот параметр, а, как мы знаем, делить на ноль нельзя! Было принято решение вернуть значение этого параметра, как было, и не трогать больше, работает ведь.

Запустил обучение, получил результаты, смотрим:

![9](https://user-images.githubusercontent.com/114439735/207909215-e963f651-9274-429e-b70b-f8053396894d.jpg)

![10](https://user-images.githubusercontent.com/114439735/207909232-f923bec6-984f-411a-81ed-3c902ebfe065.jpg)

## Задание 2
### Описать результаты, выведенные в TensorBoard.

Итак, сперва укажем, для чего вообще нужны те параметры, которые были изменены в .yaml файле. О некоторых из них я уже писал в третьей лабораторной работе.

#### buffer_size :
    Количество опыта, которое нужно набрать для обновления политики модели.
    Было увеличено.
#### beta :
    Случайность действия. Повышает разнообразие и иследованность пространства обучения.
    Было увеличено.
#### checkpoint_interval :
    Количество опыта, собираемое между каждым чекпоинтом.
    Было уменьшено.
#### max_steps :
    Количество шагов, которые должны быть выполнены в среде для завершения обучения.
    Было уменьшено.
#### time_horizon :
    Количество циклов ML-агента, необходимых перед добавлением в буфер опыта.
    Было увеличено.
#### summary_freq :
    Количество опыта, необходимого перед созданием и отображением статистики.
    Осталось без изменений.

На графиках первого обучения видно, как Cumulative Reward (накопительное вознаграждение) стремительно растёт, ведь нейросеть обучается и получает с каждым разом всё большую награду. Episode Length (длительнось эпизода) и Policy Loss (политика реагирования на неудачи) оставались стабильными. Value Loss (потеря значимости обучения) стремительно падает, из чего так же следует, что нейросеть обучается удачно. К тому же можно отметить падение Epsilon (порог расхождений между старой и новой политиками) и повышение Extrinsic Reward (награда окружения).

На удивление, второе обучение показало себя лучше. Замечу, что длительность обучения была повышена, а количество шагов удвоено. За счёт увеличения опыта, необходимого для закрепления результата (buffer_size, time_horizon), нейросеть начала показывать стабильно высокий результат уже спустя 10_000 шагов. Относительно первых графиков параметр beta стал сильнее, но всё же некритично, снижаться, хотя энтропия выросла. Epsilon так же стал снижаться сильнее.

Могу сказать, что изменения конфигурации обучения нейросети положительно сказались на итоговом результате.

## Выводы

В ходе выполненной лабораторной работы я глубже изучил параметры обучения нейросетей, познакомился с очень полезным инструментом для отображения графиков - TensorBoard. Я поражаюсь самому факту существования нейросетей, ведь в их основе заложена обыкновенная (хоть и далеко не простая) математика. С их помощью можно творить невероятные вещи, сложно представить, во что выльется их развитие в ближайшие 10 лет, но не сложно понять, что их влияние на нас будет огромным.

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
